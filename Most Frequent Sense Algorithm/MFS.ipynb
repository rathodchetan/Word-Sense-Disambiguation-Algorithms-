{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cheta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cheta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\cheta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cheta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\cheta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"semcor\") \n",
    "nltk.download(\"stopwords\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import semcor \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from string import punctuation\n",
    "from num2words import num2words\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_tagged_sents = semcor.tagged_sents(tag='sem')\n",
    "semcor_untagged_sents = semcor.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "#online loading from api\n",
    "from gensim.models import KeyedVectors\n",
    "path = \"./GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "#local loading\n",
    "# word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRA_SW = [\n",
    "    \"''\",\n",
    "    \"'s\",\n",
    "    \"``\"\n",
    "]\n",
    "\n",
    "SW = stopwords.words(\"english\")\n",
    "SW += [p for p in punctuation]\n",
    "SW += EXTRA_SW\n",
    "# if '(' in SW:\n",
    "#     print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sent):\n",
    "    sent_vec = np.zeros(300)\n",
    "    for word in sent:\n",
    "        if word in word2vec:\n",
    "            sent_vec += word2vec.get_vector(word)\n",
    "    return sent_vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses = []\n",
    "for i in range(len(semcor_tagged_sents)):\n",
    "    for j in range(len(semcor_tagged_sents[i])):\n",
    "        if isinstance(semcor_tagged_sents[i][j],nltk.Tree):\n",
    "            try:\n",
    "                if semcor_tagged_sents[i][j].height() == 3:\n",
    "                    for tree in semcor_tagged_sents[i][j]:\n",
    "                        if(tree.label() == 'NE'):\n",
    "                            senses.append('NE')\n",
    "                            \n",
    "                else:\n",
    "                    sense = semcor_tagged_sents[i][j].label().synset().name()\n",
    "                    senses.append(sense)\n",
    "                    \n",
    "            except:\n",
    "                if (semcor_tagged_sents[i][j].label()=='NE'):\n",
    "                    senses.append('NE')\n",
    "                else:\n",
    "                    sense = semcor_tagged_sents[i][j].label()\n",
    "                    senses.append(sense)\n",
    "        else:\n",
    "            senses.append('none')\n",
    "senses.append('notag')\n",
    "senses = list(set(senses))\n",
    "senses = {senses[i]:i for i in range(len(senses))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_sense_dict(tagged_sents,untagged_sents):\n",
    "    sense_dict = {}\n",
    "    for i in range(len(tagged_sents)):\n",
    "        for j in range(len(tagged_sents[i])):\n",
    "            if untagged_sents[i][j] not in sense_dict.keys():\n",
    "                sense_dict[untagged_sents[i][j]] = []\n",
    "            if isinstance(tagged_sents[i][j],nltk.Tree):\n",
    "                try:\n",
    "                    if tagged_sents[i][j].height() == 3:\n",
    "                        for tree in tagged_sents[i][j]:\n",
    "                            if(tree.label() == 'NE'):\n",
    "                                sense_dict[untagged_sents[i][j]].append('NE')\n",
    "                                \n",
    "                    else:\n",
    "                        sense = tagged_sents[i][j].label().synset().name()\n",
    "                        sense_dict[untagged_sents[i][j]].append(sense)\n",
    "                        \n",
    "                except:\n",
    "                    if (tagged_sents[i][j].label()=='NE'):\n",
    "                        sense_dict[untagged_sents[i][j]].append('NE')\n",
    "                    else:\n",
    "                        sense = tagged_sents[i][j].label()\n",
    "                        sense_dict[untagged_sents[i][j]].append(sense)\n",
    "            else:\n",
    "                sense_dict[untagged_sents[i][j]].append('none')\n",
    "    for key in sense_dict.keys():\n",
    "        sense_dict[key] = max(set(sense_dict[key]), key = sense_dict[key].count)\n",
    "    return sense_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_sense(word,sense_dict):\n",
    "    if word in sense_dict.keys():\n",
    "        return sense_dict[word]\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tagged_sents,untagged_sents,sense_dict):\n",
    "    actual = []\n",
    "    pred = []\n",
    "    count = 0\n",
    "    for i in range(len(tagged_sents)):\n",
    "        sentence = []\n",
    "        if len(tagged_sents[i])<=1:\n",
    "            continue\n",
    "        for j in range(len(tagged_sents[i])):\n",
    "            \n",
    "            word = ''\n",
    "        \n",
    "            if isinstance(tagged_sents[i][j],nltk.Tree):\n",
    "                try:\n",
    "                    if tagged_sents[i][j].height() == 3:\n",
    "                        for tree in tagged_sents[i][j]:\n",
    "                            if(tree.label() == 'NE'):\n",
    "                                \n",
    "                                word = \"_\".join(tree.leaves())\n",
    "                                actual.append('NE')\n",
    "                                \n",
    "                    else:\n",
    "                        \n",
    "                        word = \"_\".join(tagged_sents[i][j].leaves())\n",
    "                        sense = tagged_sents[i][j].label().synset().name()\n",
    "                        actual.append(sense)\n",
    "                except:\n",
    "                    if (tagged_sents[i][j].label()=='NE'):\n",
    "                        word = \"_\".join(tagged_sents[i][j].leaves())\n",
    "                        actual.append('NE')\n",
    "                    else:\n",
    "                        sense = tagged_sents[i][j].label()\n",
    "                        word = \"_\".join(tagged_sents[i][j].leaves())\n",
    "                        actual.append(sense)\n",
    "            else:\n",
    "                actual.append('none')\n",
    "                word = \"_\".join(tagged_sents[i][j])\n",
    "            pred.append(most_frequent_sense(word,sense_dict))\n",
    "        if count%500 == 0:\n",
    "            print(\"Steps Completed \",count)\n",
    "        count += 1\n",
    "        \n",
    "    return actual,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_tagged_sents = semcor.tagged_sents(tag='sem')\n",
    "semcor_untagged_sents = semcor.sents()\n",
    "sense_dict = get_most_frequent_sense_dict(semcor_tagged_sents,semcor_untagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Completed  0\n",
      "Steps Completed  500\n",
      "Steps Completed  1000\n",
      "Steps Completed  1500\n",
      "Steps Completed  2000\n",
      "Steps Completed  2500\n",
      "Steps Completed  3000\n",
      "Steps Completed  3500\n",
      "Steps Completed  4000\n",
      "Steps Completed  4500\n",
      "Steps Completed  5000\n",
      "Steps Completed  5500\n",
      "Steps Completed  6000\n",
      "Steps Completed  6500\n",
      "Steps Completed  7000\n",
      "Steps Completed  7500\n",
      "Steps Completed  8000\n",
      "Steps Completed  8500\n",
      "Steps Completed  9000\n",
      "Steps Completed  9500\n",
      "Steps Completed  10000\n",
      "Steps Completed  10500\n",
      "Steps Completed  11000\n",
      "Steps Completed  11500\n",
      "Steps Completed  12000\n",
      "Steps Completed  12500\n",
      "Steps Completed  13000\n",
      "Steps Completed  13500\n",
      "Steps Completed  14000\n",
      "Steps Completed  14500\n",
      "Steps Completed  15000\n",
      "Steps Completed  15500\n",
      "Steps Completed  16000\n",
      "Steps Completed  16500\n",
      "Steps Completed  17000\n",
      "Steps Completed  17500\n",
      "Steps Completed  18000\n",
      "Steps Completed  18500\n",
      "Steps Completed  19000\n",
      "Steps Completed  19500\n",
      "Steps Completed  20000\n",
      "Steps Completed  20500\n",
      "Steps Completed  21000\n",
      "Steps Completed  21500\n",
      "Steps Completed  22000\n",
      "Steps Completed  22500\n",
      "Steps Completed  23000\n",
      "Steps Completed  23500\n",
      "Steps Completed  24000\n",
      "Steps Completed  24500\n",
      "Steps Completed  25000\n",
      "Steps Completed  25500\n",
      "Steps Completed  26000\n",
      "Steps Completed  26500\n",
      "Steps Completed  27000\n",
      "Steps Completed  27500\n",
      "Steps Completed  28000\n",
      "Steps Completed  28500\n",
      "Steps Completed  29000\n",
      "Steps Completed  29500\n",
      "Steps Completed  30000\n",
      "Steps Completed  30500\n",
      "Steps Completed  31000\n",
      "Steps Completed  31500\n",
      "Steps Completed  32000\n",
      "Steps Completed  32500\n",
      "Steps Completed  33000\n",
      "Steps Completed  33500\n",
      "Steps Completed  34000\n",
      "Steps Completed  34500\n",
      "Steps Completed  35000\n",
      "Steps Completed  35500\n",
      "Steps Completed  36000\n",
      "Steps Completed  36500\n",
      "Steps Completed  37000\n"
     ]
    }
   ],
   "source": [
    "actual,pred = predict(semcor_tagged_sents,semcor_untagged_sents,sense_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = [senses[i] for i in actual]\n",
    "y_pred = [senses.get(i, senses['none']) for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7515550766203558\n",
      "0.6644547875469438\n",
      "0.6323030066934414\n",
      "0.7515550766203558\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "print(sm.accuracy_score(y_true, y_pred))\n",
    "print(sm.f1_score(y_true, y_pred, average='weighted'))\n",
    "print(sm.precision_score(y_true, y_pred,average='weighted'))\n",
    "print(sm.recall_score(y_true, y_pred,average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9b58781d1fa8134ac2ec6cc898ef119ce6179b951012c321b666aa023aba281"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
